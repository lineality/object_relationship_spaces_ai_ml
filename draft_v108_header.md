```
Evaluate AI with Definable Units of "Intelligence"

Object Relationship Spaces for AI ML: A Framework for 
Clearly Defined, STEM-Compatible, Project-Level, Functional Units of "Intelligence": 
For AI Design, Analysis, Performance, and Operating Systems
G.G.Ashbrook, 2023.03

Be Positive, please.

TODO:
- number and order appendices
- finish first draft of main sections
- publish to github
- publish summary on medium
- ask melanie mitchell about phd
- contact...brief history author


- add complexity section
- add mind, mindful, section
- add and elaborate on ethics areas
- make sure definition studies included, onegai
- add can-do statement to goals statement
- add appendix blurb for each obj space area


- Main examples:
	- deep blue
	- alpha go
	- go zero
	- game-playing models (find name)
	- eliza (not history, 

How to separate 2.4 Principles, Concepts & Design Factors
from background concepts in section 4...

high dimensions, low dimensions, and the formalities of manifestation
- 'what is the matrix?'

budhist 'law'
natural law
functional law
ethics

boy scounts concluding remarks?


the what is the matrix question: high and low dimensionality
	- quantum information thoery

the cutting corners shortcuts question.

for security I highly recommend security not with steve gibson and leo laporte.

topic:
- universality and general mind space
	- 
	- 


navigating mindspace: how to minds need to orient themselves
- non-automatic learning
	- blind bad cycles
- non-automatic perception
- blind bad cycles	
- mindfulness
- range of extension (parkinsons)
- empathy, compassion
- ethics, projects
- bad-attractors:
	- ideolgoies
	- super signals
	- attractive bad cycles
- blame the whistleblower


AI and Medicine: 
- Parkinson's & Alzhymers
- 





Social Uses of Technology:
- Norbert Weiner
- George Orwell
Feared that an oppressive and non-democratic government could use technology to suppress and manipulate people in cynical ways. 

How should we try to safeguard technology from being used against society (and of course, whoever does so will claim they are doing it for society and very well may have populist support behind them). 

As Micheal Wooldrich says, AI is wonderfully interdiscplinary. So, challenge: How can we make sure we are including input from the various fields that AI should be getting input from? 


Focusing on Input Output Metrics, non-automatic learning, categories of types of systems, and general system collapse, 


System-1 and System-2 type human thinking.


a summary of the hypothetico deductive method


extend your discussion and reading. Read new authors like Michael Pollen and Rupert sheldrake, intelligent people who have thought long and hard about difficult topics, and can help you distill what would be many times over more than a lifetime's research across different fields. And read older writers such as Dogen, a Japanese writer about mindfulness from the 1200's. Yet, it appears many things about the human mind are just as they were then. 

Move it to 4

Audio:
https://drive.google.com/drive/folders/1b4ODYFt7R7AgYG5u7yXap7f3Zl8b-HXI 


Abstract
There is a need for the use of well defined performance frameworks to describe the goals and abilities of systems including AI. 

The object-relationship-space framework can be used to guide project-specific design, identifying components, to guide discussion, analysis, testing, and reporting, and possibly to make operating systems to manage and enable smaller or larger scale AI projects coordinating required abilities and internal and external components, including symbolic logistics and sub-symbolic training for AI-self-management. 

Project-specific points in the framework are equal to points in a matrix of object-relationship(space)-vectors, which in future may be used with solutions found and navigated by AI.

Clearly describing a skill that a given AI does not yet have can be a significant part of developing that yet-to-be-attained skill.

Part one concerns a brief overview of the framework.

Part two concerns using the framework and networking AI components.

Part three concerns a discussion of the discussion of AI.

Part four concerns goals, background, and the future.

Appendices include more examples and details.



Table of Contents:

Abstract
	(above)

A Narrative Introduction

Part 1: The Object Relationship Space Framework
1.1  Example General Object Relationship Space List
1.2  Many lists in One
1.3  Networked-AI Components

Part 2: Using The Framework
2.1  Examples Mapped to Object Spaces
2.2  Adding Levels, Adding Steps, Adding Objects
2.3  Heuristic & Pseudocode for AI Management with Object Spaces
2.4  Modularity, Scale, and Networks
		- whole ai, networked ai, ecosystems

Part 3: Discussing the Discussion of AI

3.1 Define Your Terms
3.2 Define Your Project Context
Is AI you are considering correct for your system?
3.3 evaluating claims
3.4 A Three Legged Writing Stool
Cautionary Tales:
- Chess
- The IQ Test
 	3.5 misrepresenting humans in general
	3.6 problems of popular science & juggling audiences
		ELIZA
	3.7 beware the traps:
		- bad reasoning (back to greeks, biases etc.)
		- bad definitions
		- bad context
		- supersignals
		- demand distortion
		- disinformation
		- indeterminate incompetance and malice
	
	Main books, main sited examples:
		- 
	Concepts to look at carefully

	input ouptut measures...or next section?
	- 

Part 4:  Goals, Background & Future: Principles, Concepts & Design Factors
	4.1  Goals Statements
	4.2  AI Concepts and Principles
		- Correcting The Misdirection: 
incorrect parts of the standard narative.
		- Extra Needed Concepts
			- natural law
			- 
		- Projects
	
	4.3  Background and Bigger Picture Topics:
		- Mind
		- Education
		- Learning about People By Learning How Machines Learn
		- Universality
		- Duty & Responsibility
		- Pathology & Healing
		- 
	4.5  The Future

	higher order concepts in matrix space vs. 'understanding'
	


Appendices:
Appendix #: Recommended Reading & Extended Reading
Appendix #: Expanded Introduction
Appendix #: Expanded Part 1
Appendix #: Expanded Part 2
Appendix #: Expanded Part 3
Appendix #: Expanded Part 4


Appendix #: A more ground-up explanation and discussion of objection-relationship-space with examples.
Appendix #: Framework Heuristic Pseudo Code Walk-Through Examples and Discussion
			- The Tea Story
			- A Bicycle Story
Appendix #: Pseudocode Implementation Examples
			- dialogue meeting targets
			- dialogue with Pseudocode
Appendix #: More Details and Illustrations
- Networked-AI(parts)
- The Tea Story
- A Birthday Story
Appendix #: Illustrating AI Functions
Appendix #: Critiques of AI Literature
Appendix #: Wolfram Alpha AI Test Examples
Appendix #: ChatGPT Test Examples
Appendix #: Other AI Test Examples
Appendix #:...
		- gamification
		- 
Appendix #: Other Notes


question-space and asking questions









A Narrative Introduction


If I were going to read another book about AI, and it was exactly the book I was looking for, what would I like that book to be about? 

I love the history. Everyone covers different bits of the history, and I absolutely love hearing about all the different anecdotes and personalities and the stories and discoveries. I will probably try to make my own super-collection of the history of AI by combining everything that I can find everywhere.  

Then there are the main parts of AI that everyone covers. The main parts are pretty much the same in every book and it's always good to get a few new details and perspectives about these main models and technologies that have emerged. Because everyone covers them in a slightly different way. And you can always get a juicy new lead to follow, some new person or aspect you didn't know about before. 

The part that I...don't want, I'm not sure if there is an entirely-positive way of saying this, is all the weird misdirection stuff. And there are at least two parts to that. One is the profoundly incorrect statements about people. Like: 
'People learn instantly.' 
'They learn automatically and don't need to be taught.' 
'They adapt instantly to any new situation.' 
'They can take one bit of learning and instantly apply it to thousands of completely novel situations they have had absolutely no experience with before.'
All of that is garbage. 

The other part is the super-weird fetishism around 'general-intelligence.' And the two are being intertwined into a cycle of nonsense: 
"AI is not true AI because it is not like people." 
"People have general-intelligence, and AI isn't intelligent because it doesn't have the human-general-intelligence." 
"And if AI did then it could do anything, instantly, automatically, and transfer instantly to anything else, just like people!" 
That narrative is not helpful. That our goal is to make this meta-abstract general-intelligent-AI that follows this impossible fictional definition of 'people,' is a complete departure from reality. 

Hearing people talking about this over and over again in all these books was how I started coming up with this concept of Object-Relationship-Spaces. I was trying to figure out more specifically "What can AI not do?", instead of just saying over and over again, "It's not general!" in a circle that does not accomplish anything. 

For example (there are lots of examples in the appendix), we can ask questions and see what specific things, what specific objects and types of objects, the AI is, or is not, able to deal with. Is it able to recognize them? Is it able to see how they relate to other things ("relationships")? Is it able to combine them?

So, the idea is using objects, types of object, relationships between types of objects, as a framework for looking at what AI can do and cannot do. And if you hit a wall where you really want AI to do something, but it can't do it, then you could figure out how to do it. For example, make a general AI operating system that breaks things down into objects and basically has a database of objects that it is dealing with in the given context, not a knowledge-base about the whole universe, but objects that are specific to the project you are trying to do: an external project-object-database. 

Part of what to me seemed to be the low hanging fruit, and this is the way the original paper started, is around the phrase, "AI is very good at identifying things, but it's not very good at doing anything with that information." 

You can show an AI two pictures and it can correctly identify exactly what sub-species of animal they are, but ask it something about the pictures like: Did I just show you two pictures? And it can't answer because it has no 'object space.' It has no problem-space or project-space, or object-space to use the same information that it just gave you. It literally has the answer. It just gave you the answer. But it can't project-manage that same correct answer as an object in object-space. 

If you show it two cat pictures and ask if those two pictures you showed it were both identified the same way (both 'cat'), it has no way of processing that question at all: In the extreme if it is only a picture-input AI then you can't ask it anything in language at all. The input is one picture. The output is one word. It can't do anything else. There is no language object-space. And most AI historically have been this kind of hyper-focused single-purpose AI that has one very narrow type of input and just one very narrow output, like cat-picture classification.

The standard comment in books and articles is something like this, paraphrased here: 
An AI can identify a picture as a cat, 
but it doesn't "understand" what a cat is. 
And the discussion of this problem stays in that circular space of:
- human-ness
- general-intelligence
- world-knowledge-base
- common sense
- automatic-transference-of-learning
- general-understanding

But let's focus on the problem here. If you show a picture-identifying-AI two pictures, the only things that exist in that AI's world, in the problem space, are two things: the two answers it just gave you. If you want to ask the AI if those two answers are the same (both 'cat'), not only does the AI have the answer, the only things it has are the answer. And the route to getting the AI to answer logical questions about the answers it gave you is not to give the AI infinitely more "knowledge base" and "common sense" knowledge about the world at large: the AI already has the answer, the problem is that the AI cannot deal with objects. If an AI can do anything, it can answer logical questions about the objects that are already in its problem space. 
How many things are in your problem space. -> Two.
Are they the same? -> Yes.
Trying to teach the AI that cats have feelings and that cat-people and dog-people often watch different movies isn't going to somehow give the AI the ability to manage the project-objects that it already has. 

Part of why I took that line out of the paper is that OpenAI's ChatGPT (released in late 2022) is so much better at parts of this task of dealing with objects that it is no longer factually correct to say "AI cannot deal with objects." We now have one that can. And I've never seen that discussed in any articles about ChatGPT, even by OpenAI. Most news articles (like this one: ) unsurprisingly say exactly the same standard line: chatGPT isn't really intelligent because it lacks understanding, it doesn't have general-intelligence because it isn't like people, and on and on in that same cycle leading nowhere. 

Interestingly, ChatGPT cannot deal with pictures, it strictly deals with text. So whether it could connect a picture to text, we don't know. But amazingly, it not only has the logical problem space skills we were just talking about, it also has a fair amount of common-sense world knowledge-base information. 

One of the things that is important for this discussion of how we should go about designing AI is that arguably my proposed idea of having a brute force project-object-database is too brute force for many tasks. We would want the AI do a lot of things on its own. It wouldn't need to analytically separate all the objects and step through all steps. People tried what is called 'symbolic' AI, hand-crafted logical steps for everything, we tried for many years and it doesn't work very well. However, aside from the information processing that goes on inside the AI (e.g. how it identifies the picture as a cat), but if we are talking about a project, if you have an AI working on a project then it needs to be able to communicate and coordinate about everything that is in an external project-object-database with all the other people and other AI who are working on that project. So that idea of having an external project-object-database ends up still being there for building and managing AI projects, even though it may not be used at all for how the AI does anything internally as it processes information. Though I still think that in some very specific cases, because most AI is micro-specific AI not super-grand mega-AI, that if you had for example a local AI running on a tiny resource limited micro-controler computer, maybe in a factory or warehouse somewhere, and it was having trouble dealing with an object in its very specific task, then why not try having it use a project-object database to help it process and deal with things. 

This has wandered off the topic of books that I would like to read. I think it would be really neat if a book talked about possible ideas in the future:
- How is AI going to integrate with Biology?
- How does AI relate to education, governance, legal systems, STEM & Science?
- How will AI be directly and indirectly integrated into the human body?
- How might AI be used during space programs?
And general mind discussion. You have the human mind, you have, theoretically, Extra Terrestrial species that have minds, and then AI-mindspace. And then you have all the other mind-space topics that we are not supposed to talk about: sacred plants, afterlife experiences, meditation, mindfulness, compassion, ethics, boy scout values: duty, responsibility, conscience. I think we should talk about those things. I would love to read about those things. 

How universal is mindspace? There is only one species with language and self-reflecting consciousness on earth. Is every intelligent (Drake Equation) species going to have some behavioral problems similar to those of humans? How many of the weird problems that people have are just part of universal mindspace? Mindspace is difficult, for example the 'curse of dimensionality.' Dealing with higher and lower dimensions is probably difficult for anyone anywhere, not just for some people or species or organizations. As we build up our general model of mindspace, what are the things that we think are universal? What are the things we think are not universal, and which are things we just don't know? This will be important both for AI and just managing societies and facilitating human development. 

What I would put into that discussion are things like general system collapse, and categories of types of systems. I suspect there are a small number of things that are very universal, as in probably in any universe: some aspects of STEM; some aspects of categories of types of systems; and general system and definition collapse processes. Even if that is the only skeleton of universality that would allow us to communicate with other species or even be the best way to interact with AI if it thinks very differently from how we do, then that may be the low hanging fruit that we can start with. 

And hopefully the better we understand the basics of mindspace, the better foundation we will have in many areas including properly managing AI. 
```
